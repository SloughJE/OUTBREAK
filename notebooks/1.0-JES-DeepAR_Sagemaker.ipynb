{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ec312c-35da-41ce-b153-f09c819b976e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install boto3 pandas pyathena pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd372d3-a579-44d6-af2c-8aea2146d536",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "import s3fs\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ba337f-8243-43d8-8a5e-80b5603cfac9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# at this point, I will have predictions from last week for current data\n",
    "# and current actuals\n",
    "# need to merge the latest weekly data with the predictions from last week\n",
    "# how to get the latest weekly_actuals?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974bf6ea-80c1-46f8-b19b-2014459c41f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a722d9-d467-41ce-911e-27fe792c461a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the Boto3 Athena client\n",
    "athena_client = boto3.client('athena', region_name='us-east-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772100ab-aefe-4372-9908-9021f643e61e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# combine historical and latest data\n",
    "#sql_query = \"\"\"\n",
    "#SELECT * FROM (\n",
    "#    SELECT item_id, year, week, date, label, new_cases\n",
    "#    FROM historical\n",
    "#    UNION ALL\n",
    "#    SELECT item_id, year, week, date, label, new_cases\n",
    "#    FROM weekly\n",
    "#) AS combined_data\n",
    "#\"\"\"\n",
    "\n",
    "sql_query = \"\"\"\n",
    "SELECT item_id, year, week, date, label, new_cases \n",
    "FROM weekly\n",
    "\"\"\"\n",
    "\n",
    "# Specify the Athena database and S3 output location\n",
    "database = 'cdc_nndss'\n",
    "s3_output_constant = 's3://nndss/query_results/weekly_combined_data'\n",
    "\n",
    "# Adjust the ResultConfiguration to use the constant output location\n",
    "response = athena_client.start_query_execution(\n",
    "    QueryString=sql_query,\n",
    "    QueryExecutionContext={'Database': database},\n",
    "    ResultConfiguration={'OutputLocation': s3_output_constant}\n",
    ")\n",
    "\n",
    "# Get the query execution ID\n",
    "query_execution_id = response['QueryExecutionId']\n",
    "\n",
    "# Function to check the query execution status\n",
    "def wait_for_query_completion(client, query_id):\n",
    "    while True:\n",
    "        response = client.get_query_execution(QueryExecutionId=query_id)\n",
    "        state = response['QueryExecution']['Status']['State']\n",
    "        if state in ['SUCCEEDED', 'FAILED', 'CANCELLED']:\n",
    "            return state\n",
    "        time.sleep(5)\n",
    "\n",
    "# Wait for the query to complete\n",
    "query_state = wait_for_query_completion(athena_client, query_execution_id)\n",
    "if query_state == 'SUCCEEDED':\n",
    "    print(f\"Query completed successfully. Results are stored in the directory: {s3_output_constant}/{query_execution_id}/\")\n",
    "elif query_state in ['FAILED', 'CANCELLED']:\n",
    "    print(f\"Query did not complete successfully. State: {query_state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32911c46-c267-4e4f-be50-b96bbcbb8372",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_file_path = f\"{s3_output_constant}/{query_execution_id}.csv\"\n",
    "df = pd.read_csv(result_file_path)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5538b5a-eaf2-4e34-a448-7173f0cc33a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert 'date' to the appropriate datetime format if not already\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Sort the DataFrame by 'item_id' and 'date' to ensure the order of the time series\n",
    "df.sort_values(by=['item_id', 'date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9987b8-090b-456f-86c1-d5b379b77ba8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df = df[df.date<pd.to_datetime(\"2024-03-11\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0910a2-0f90-4012-828a-aa837364a199",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[df.item_id=='WYOMING_Zika virus disease, non-congenital']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e99729c-e72c-427f-8d5c-412b696169b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "output_bucket = 'nndss'\n",
    "output_key = 'deepar_input_data/deepar_dataset.jsonl'\n",
    "s3_output_path = f's3://{output_bucket}/{output_key}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8624ee-730f-488c-a80f-5b0ac48b43a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A function to convert NaN values to \"NaN\" string and others to float\n",
    "def convert_target(target_series):\n",
    "    return [float(x) if pd.notna(x) else \"NaN\" for x in target_series]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3243d968-ac81-4908-a338-f5c5d5b110cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "time_series_mapping = {}  # To store the mapping of item_id to its index in the JSON Lines file\n",
    "json_lines = []  # To store the JSON Lines\n",
    "\n",
    "for idx, (item_id, group) in enumerate(df.groupby('item_id')):\n",
    "    time_series = {\n",
    "        \"start\": str(group['date'].dt.date.iloc[0]),  # Assuming the 'date' column is already a datetime\n",
    "        \"target\": convert_target(group['new_cases']),\n",
    "    }\n",
    "    json_lines.append(json.dumps(time_series))\n",
    "    time_series_mapping[item_id] = idx  # Map item_id to its index in the JSON Lines file\n",
    "\n",
    "# Convert JSON Lines list to a single string\n",
    "json_lines_str = \"\\n\".join(json_lines)\n",
    "\n",
    "# Define S3 keys for the JSON Lines file and the mapping file\n",
    "json_lines_key = 'deepar_input_data/deepar_dataset.jsonl'\n",
    "mapping_key = 'deepar_input_data/time_series_mapping.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c7151a-ce4e-4b66-bfa7-12ebd265eb23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the JSON Lines file to S3\n",
    "s3.Object(output_bucket, json_lines_key).put(Body=json_lines_str)\n",
    "\n",
    "# Save the mapping file to S3\n",
    "mapping_str = json.dumps(time_series_mapping)\n",
    "s3.Object(output_bucket, mapping_key).put(Body=mapping_str)\n",
    "\n",
    "print(f\"JSON Lines file saved to s3://{output_bucket}/{json_lines_key}\")\n",
    "print(f\"Mapping file saved to s3://{output_bucket}/{mapping_key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a371beb-10ec-468c-b935-d58f03376721",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(time_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1a375a-3119-4daf-8369-fd5756904cbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import image_uris, Session\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.session import get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa71f76-2898-419b-b459-b8e7c2d50ced",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "role = sagemaker.get_execution_role()  # IAM role to use by SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264ac39a-9954-4069-83eb-44b98d6fd61f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "container = image_uris.retrieve('forecasting-deepar', region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e652be5d-c916-4ba7-a107-8585d6e6ed2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure the estimator\n",
    "deepar = Estimator(\n",
    "    container,\n",
    "    role,\n",
    "        use_spot_instances=True,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.c4.xlarge',\n",
    "    output_path=f's3://{output_bucket}/deepar/output',\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d665c25-119b-4b9f-a9e6-fd1183f502e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "deepar.set_hyperparameters(\n",
    "    time_freq='W',\n",
    "    epochs=20,\n",
    "    early_stopping_patience=10,\n",
    "    prediction_length=1,\n",
    "    context_length=1,\n",
    "    num_cells=40,\n",
    "    num_layers=2,\n",
    "    mini_batch_size=64,\n",
    "    learning_rate=0.001,\n",
    "    dropout_rate=0.05,\n",
    "    likelihood='negative-binomial'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b234bc2a-a27b-4a08-80df-9f3d682db781",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify data channels\n",
    "data_channels = {\n",
    "    'train': f's3://{output_bucket}/deepar_input_data/deepar_dataset.jsonl',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfc3496-b06c-4523-8e45-c411d14492ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start training\n",
    "deepar.fit(inputs=data_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97cda1e-1da0-41a9-b5ba-9a18500cf1e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "bucket_name = 'nndss' \n",
    "key = 'deepar_input_data/deepar_dataset.jsonl'  \n",
    "\n",
    "obj = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "data = obj['Body'].read().decode('utf-8')\n",
    "deepar_training = data.strip().split('\\n')\n",
    "deepar_training = [json.loads(line) for line in deepar_training]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa96e59-0434-4bef-a336-8c136e667c34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "predictor = deepar.deploy(\n",
    "    initial_instance_count=1,  # Number of instances to support the endpoint\n",
    "    instance_type='ml.m4.xlarge',  # Type of instance to run the endpoint\n",
    "    serializer=JSONSerializer(),  # Specify how to serialize the input data\n",
    "    deserializer=JSONDeserializer()  # Specify how to deserialize the prediction output\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bb19b9-fcef-4d0a-80d1-51a2d82c3e8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preparing predictor input\n",
    "predictor_input = {\n",
    "    \"instances\": deepar_training,\n",
    "    \"configuration\": {\n",
    "        \"num_samples\": 100,\n",
    "        \"output_types\": [\"mean\", \"quantiles\"],\n",
    "        \"quantiles\": [\"0.01\", \"0.5\", \"0.99\"]\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8e2bd0-72cd-4e1f-973b-68d8c97e5cef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prediction = predictor.predict(predictor_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f9c9c7-4e74-4157-870e-8b979c985884",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3074f1b8-7a92-4125-9f03-984687b5ddd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def find_max_date(deepar_training):\n",
    "    latest_dates = []\n",
    "    for series in deepar_training:\n",
    "        start_date = datetime.strptime(series['start'], \"%Y-%m-%d\")\n",
    "        # Assuming weekly frequency, calculate the end date of each series\n",
    "        end_date = start_date + timedelta(weeks=len(series['target']) - 1)\n",
    "        latest_dates.append(end_date)\n",
    "    \n",
    "    # Find the maximum date across all series, which is the last known date in the dataset\n",
    "    max_date = max(latest_dates)\n",
    "    return max_date\n",
    "\n",
    "# Use the function to find the last known date in training data\n",
    "last_known_date = find_max_date(deepar_training)\n",
    "\n",
    "# The prediction_for_date is the next time period (e.g., the next week) after the last known date\n",
    "prediction_for_date = pd.to_datetime(last_known_date + timedelta(weeks=1))\n",
    "print(f\"The prediction is for the date: {prediction_for_date.strftime('%Y-%m-%d')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf333641-f8d8-4269-be0e-f4844f121770",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Initialize a list to store prediction data\n",
    "prediction_data = []\n",
    "\n",
    "# Iterate through each prediction and its corresponding item_id\n",
    "for idx, pred in enumerate(prediction['predictions']):\n",
    "    # Retrieve the item_id using the index\n",
    "    item_id = list(time_series_mapping.keys())[list(time_series_mapping.values()).index(idx)]\n",
    "    \n",
    "    # Extract quantiles\n",
    "    pred_lower = pred['quantiles']['0.01']\n",
    "    pred_upper = pred['quantiles']['0.99']\n",
    "    pred_median = pred['quantiles']['0.5']\n",
    "    pred_mean = pred['mean'] \n",
    "    \n",
    "    # Append the data to the list\n",
    "    prediction_data.append({\n",
    "        'item_id': item_id,\n",
    "        'pred_mean': pred_mean,\n",
    "        'pred_median':pred_median,\n",
    "        'pred_lower': pred_lower,\n",
    "        'pred_upper': pred_upper\n",
    "    })\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "prediction_df = pd.DataFrame(prediction_data)\n",
    "prediction_df['prediction_for_date'] = prediction_for_date\n",
    "prediction_df['pred_mean'] = prediction_df['pred_mean'].apply(lambda x: x[0] if x else None)\n",
    "prediction_df['pred_median'] = prediction_df['pred_median'].apply(lambda x: x[0] if x else None)\n",
    "prediction_df['pred_lower'] = prediction_df['pred_lower'].apply(lambda x: x[0] if x else None)\n",
    "prediction_df['pred_upper'] = prediction_df['pred_upper'].apply(lambda x: x[0] if x else None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384cdd49-843f-4938-8d70-d7fdeab35ae0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prediction_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae8ff43-187e-41c1-a924-48a389f92b2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prediction_df[prediction_df.pred_mean>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef6fcf6-1c25-4b21-8b1c-d15b7efac09a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(prediction_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f44d94b-5911-4e46-9b79-aa95d17a1232",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prediction_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc829be4-b332-4ce9-8310-f9d1774371e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify your S3 bucket and path\n",
    "bucket_name = 'nndss'\n",
    "folder_path = 'predictions'\n",
    "file_name = f\"weekly_predictions_{prediction_for_date.strftime('%Y-%m-%d')}.parquet\"\n",
    "s3_path = f's3://{bucket_name}/{folder_path}/{file_name}'\n",
    "\n",
    "# Save DataFrame to Parquet directly in S3\n",
    "prediction_df.to_parquet(s3_path, engine='pyarrow', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e50c33-ef95-4d70-8360-01b12ed37ae4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84d3a7d-26de-4a60-955a-da6961f17d80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# here I would dump the latest weekly data (from weekly_staging) into weekly, and delete the file in weekly_staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb209a8-aed0-4572-8f21-048b9325f6b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
