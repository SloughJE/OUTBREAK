{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "from itertools import product\n",
    "from datetime import date\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok, so in the csv, the year / week is just ['Current MMWR Year','MMWR WEEK'], and the number of cases (new cases) is \"CUrrent week\", Label is the disease.\n",
    "# and similarly the api response, states\tyear\tweek\tlabel\tm1 (m1 is the current week number of new cases)\n",
    "# states or reporting area contains more than states. regions, total, territories, US res, non-us res\n",
    "# thought is to, create an outlier detection thing like in time series stuff I did for Live action. I think it's possible.\n",
    "# https://dev.socrata.com/foundry/data.cdc.gov/x9gk-5huc\n",
    "# now that I've download from API the data, it's better formatted. for no data it's actually NaN\n",
    "# so in short, i'm filling forward fillin in all dates (weeks of year) that aren't in the actual data but adding a flag to id that so if we later want to have a cumulative count\n",
    "# we can ignore those rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../data/raw/NNDSS.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['states','year','week','label','m1','location1']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['location1'].notna()] # removes regions and USA total\n",
    "df = df.drop(columns='location1')\n",
    "df.columns = ['state','year','week','label','new_cases']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'date' column is in the correct format and is sorted\n",
    "df['date'] = pd.to_datetime(df['year'].astype(str) + '-' + df['week'].astype(str) + '-1', format='%Y-%W-%w')\n",
    "# make unique id with state and label:\n",
    "df['item_id'] = df['state'] + '_' + df['label']\n",
    "df.sort_values(['item_id', 'date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['new_cases'] = df.groupby('item_id')['new_cases'].transform(lambda x: x.ffill().bfill().fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['new_cases'] = df.new_cases.astype(int)\n",
    "df['week'] = df.week.astype(int)\n",
    "df['year'] = df.year.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df.label.str.contains(\"Probable\")] # remove 'probable' as I don't want to predict probable diseases, only confirmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weeks_in_year(year):\n",
    "    \"\"\"Determine the number of ISO weeks in a given year.\"\"\"\n",
    "    last_day_of_year = date(year, 12, 28)  # ISO-8601; the week containing 28th Dec is the last week of the year\n",
    "    return last_day_of_year.isocalendar()[1]\n",
    "\n",
    "def fill_weekly_gaps(df):\n",
    "    # Determine the maximum year and week present in the data for later use\n",
    "    max_year = df['year'].max()\n",
    "    max_week_for_max_year = df[df['year'] == max_year]['week'].max()\n",
    "\n",
    "    all_combinations = []\n",
    "    for item_id in tqdm(df['item_id'].unique(), desc='Filling gaps'):\n",
    "        item_df = df[df['item_id'] == item_id]\n",
    "        first_year = item_df['year'].min()\n",
    "        last_year = item_df['year'].max()\n",
    "        first_week = item_df[item_df['year'] == first_year]['week'].min()\n",
    "        last_week = item_df[item_df['year'] == last_year]['week'].max()\n",
    "        \n",
    "        for year in df['year'].unique():\n",
    "            if year < first_year or year > last_year:\n",
    "                continue  # Skip years before the item_id first appears and after it last appears\n",
    "            \n",
    "            week_start = first_week if year == first_year else 1\n",
    "            week_end = last_week if year == last_year else get_weeks_in_year(year)\n",
    "            for week in range(week_start, week_end + 1):\n",
    "                all_combinations.append({'item_id': item_id, 'year': year, 'week': week})\n",
    "                \n",
    "    all_combinations_df = pd.DataFrame(all_combinations)\n",
    "\n",
    "    # Merge the generated combinations with the original DataFrame\n",
    "    df_merged = pd.merge(all_combinations_df, df, on=['item_id', 'year', 'week'], how='left', indicator=True)\n",
    "    \n",
    "    # Carry forward the last observed 'new_cases', but only within the bounds of existing data for each item_id\n",
    "    df_merged['new_cases'] = df_merged.groupby('item_id')['new_cases'].ffill().bfill()\n",
    "\n",
    "    # Mark filled values for new_cases\n",
    "    df_merged['filled_value'] = df_merged['_merge'] == 'left_only'\n",
    "    df_merged.drop(columns=['_merge'], inplace=True)\n",
    "\n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fill_weekly_gaps(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.item_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_week_to_date(year, week):\n",
    "    \"\"\"\n",
    "    Convert a year and week number into the date of the Monday of that week.\n",
    "    \"\"\"\n",
    "    # Calculate the first day of the year\n",
    "    first_of_year = datetime(year, 1, 1)\n",
    "    # ISO-8601 calculation for the first week of the year\n",
    "    if first_of_year.weekday() > 3:  # If the first day is Friday or later\n",
    "        # Move to the next Monday\n",
    "        first_of_year += timedelta(days=7-first_of_year.weekday())\n",
    "    else:\n",
    "        # Move to the Monday of the current week\n",
    "        first_of_year -= timedelta(days=first_of_year.weekday())\n",
    "    \n",
    "    # Calculate the Monday of the given week number\n",
    "    week_start_date = first_of_year + timedelta(weeks=week-1)\n",
    "    \n",
    "    return week_start_date\n",
    "\n",
    "# Assuming df is your DataFrame and it already contains 'year' and 'week' columns\n",
    "# Update the 'date' column with the calculated Monday dates\n",
    "df['date'] = df.apply(lambda row: year_week_to_date(int(row['year']), int(row['week'])), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['new_cases'] = df.new_cases.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in the state and label for the inserted rows if needed later\n",
    "df['state'] = df.groupby('item_id')['state'].ffill().bfill()\n",
    "df['label'] = df.groupby('item_id')['label'].ffill().bfill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mod = df[['item_id','date','label','new_cases']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mod.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the cut-off date for the train-test split\n",
    "# For example, if you want the last 4 weeks as your test set:\n",
    "cut_off_date = df_mod['date'].max() - pd.Timedelta(weeks=10)\n",
    "print(cut_off_date)\n",
    "# Splitting the DataFrame into training and testing sets\n",
    "train = df_mod[df_mod['date'] <= cut_off_date]\n",
    "test = df_mod[df_mod['date'] > cut_off_date]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.dataset.pandas import PandasDataset\n",
    "from gluonts.dataset.split import split\n",
    "from gluonts.torch import DeepAREstimator\n",
    "from gluonts.dataset.common import ListDataset\n",
    "from gluonts.torch.distributions.negative_binomial import NegativeBinomialOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = PandasDataset.from_long_dataframe(train, target='new_cases', item_id='item_id', \n",
    "                                             timestamp='date', freq='W')\n",
    "test_ds = PandasDataset.from_long_dataframe(test, target='new_cases', item_id='item_id', \n",
    "                                            timestamp='date', freq='W')\n",
    "                                            \n",
    "# Train the model and make predictions\n",
    "estimator = DeepAREstimator(\n",
    "    prediction_length=10,\n",
    "    freq=\"W\",\n",
    "    distr_output=NegativeBinomialOutput(),\n",
    "    trainer_kwargs={\"max_epochs\": 100}\n",
    ")\n",
    "\n",
    "model = estimator.train(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = list(model.predict(train_ds))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.tseries.offsets import Week\n",
    "\n",
    "start_date = test.date.min()\n",
    "num_periods = 10\n",
    "prediction_dates = pd.date_range(start=start_date, periods=num_periods, freq='7D')\n",
    "prediction_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import Timestamp\n",
    "\n",
    "all_preds = []\n",
    "for i, forecast in enumerate(preds):\n",
    "    item_id = forecast.item_id\n",
    "    pred_df = pd.DataFrame({\n",
    "        'date': prediction_dates,\n",
    "        'item_id': item_id,\n",
    "        'pred_mean': forecast.mean,\n",
    "        'pred_lower': forecast.quantile(0.01),\n",
    "        'pred_upper': forecast.quantile(0.99)\n",
    "    })\n",
    "    all_preds.append(pred_df)\n",
    "\n",
    "all_preds_df = pd.concat(all_preds, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evaluation = pd.merge(all_preds_df, test, on=['item_id', 'date'])\n",
    "df_evaluation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate errors\n",
    "mae = mean_absolute_error(df_evaluation['new_cases'], df_evaluation['pred_mean'])\n",
    "rmse = np.sqrt(mean_squared_error(df_evaluation['new_cases'], df_evaluation['pred_mean']))\n",
    "\n",
    "# Since MAPE can have division by zero issues, we'll handle it carefully\n",
    "mape = np.mean(np.abs((df_evaluation['new_cases'] - df_evaluation['pred_mean']) / df_evaluation['new_cases'].replace(0, np.nan))) * 100\n",
    "\n",
    "# Printing the errors\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"Mean Absolute Percentage Error (MAPE): {mape}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_new_cases = df_evaluation.groupby('item_id')['new_cases'].apply(lambda x: x.mode()[0])\n",
    "item_ids_to_keep = mode_new_cases[mode_new_cases != 0].index\n",
    "filtered_df = df_evaluation[df_evaluation['item_id'].isin(item_ids_to_keep)]\n",
    "mae = mean_absolute_error(filtered_df['new_cases'], filtered_df['pred_mean'])\n",
    "rmse = np.sqrt(mean_squared_error(filtered_df['new_cases'], filtered_df['pred_mean']))\n",
    "\n",
    "# Since MAPE can have division by zero issues, we'll handle it carefully\n",
    "mape = np.mean(np.abs((filtered_df['new_cases'] - filtered_df['pred_mean']) / filtered_df['new_cases'].replace(0, np.nan))) * 100\n",
    "\n",
    "# Printing the errors\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"Mean Absolute Percentage Error (MAPE): {mape}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "\n",
    "def plot_forecasts_plotly(df, all_preds_df, num_charts=4):\n",
    "    item_ids = np.random.choice(df['item_id'].unique(), size=num_charts, replace=False)\n",
    "    \n",
    "    num_rows = num_charts // 2 + num_charts % 2\n",
    "    fig = make_subplots(rows=num_rows, cols=2, subplot_titles=item_ids)\n",
    "    \n",
    "    for i, item_id in enumerate(item_ids, start=1):\n",
    "        original_filtered = df[df['item_id'] == item_id]\n",
    "        predictions_filtered = all_preds_df[all_preds_df['item_id'] == item_id]\n",
    "\n",
    "        row = (i-1) // 2 + 1\n",
    "        col = i % 2 if i % 2 != 0 else 2\n",
    "\n",
    "        # Plot the actual values with lines\n",
    "        fig.add_trace(go.Scatter(x=original_filtered['date'], y=original_filtered['new_cases'],\n",
    "                                 mode='lines+markers', name=f'Actual {item_id}',\n",
    "                                 legendgroup=f\"group{i}\", showlegend=False,\n",
    "                                 line=dict(color='blue'),  # Set the line color to blue\n",
    "                                 marker=dict(color=original_filtered['filled_value'].map({True: 'green', False: 'blue'}),  # Color dots based on filled_value\n",
    "                                             size=2)),  # Set marker size\n",
    "                      row=row, col=col)\n",
    "\n",
    "        # Plot the predicted mean\n",
    "        fig.add_trace(go.Scatter(x=predictions_filtered['date'], y=predictions_filtered['pred_mean'],\n",
    "                                 mode='lines', name=f'Predicted {item_id}',\n",
    "                                 legendgroup=f\"group{i}\", showlegend=False, line=dict(color='red')),\n",
    "                      row=row, col=col)\n",
    "\n",
    "        # Prediction intervals\n",
    "        fig.add_trace(go.Scatter(x=predictions_filtered['date'], y=predictions_filtered['pred_lower'],\n",
    "                                 mode='lines', name=f'Lower {item_id}', \n",
    "                                 legendgroup=f\"group{i}\", showlegend=False, line=dict(width=0)),\n",
    "                      row=row, col=col)\n",
    "\n",
    "        fig.add_trace(go.Scatter(x=predictions_filtered['date'], y=predictions_filtered['pred_upper'],\n",
    "                                 mode='lines', name=f'Upper {item_id}', fill='tonexty',\n",
    "                                 legendgroup=f\"group{i}\", showlegend=False, line=dict(width=0), fillcolor='rgba(255, 0, 0, 0.3)'),\n",
    "                      row=row, col=col)\n",
    "        \n",
    "    fig.update_layout(template=\"plotly_dark\", height=300*num_rows, title_text=\"Forecasts of New Cases\", showlegend=False)\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_forecasts_plotly(df, all_preds_df, num_charts=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg = df.groupby(['item_id','state','label'],as_index=False)['new_cases'].mean()\n",
    "df_avg[df_avg.new_cases>50].item_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_forecasts_plotly_by_item_id(df, all_preds_df, item_ids, num_charts=None):\n",
    "    # If num_charts is not specified, plot for all given item_ids\n",
    "    if num_charts is None:\n",
    "        num_charts = len(item_ids)\n",
    "    else:\n",
    "        num_charts = min(num_charts, len(item_ids))\n",
    "    \n",
    "    item_ids_to_plot = np.random.choice(item_ids, size=num_charts, replace=False) if len(item_ids) > num_charts else item_ids\n",
    "    \n",
    "    num_rows = num_charts // 2 + num_charts % 2\n",
    "    fig = make_subplots(rows=num_rows, cols=2, subplot_titles=item_ids_to_plot)\n",
    "    \n",
    "    for i, item_id in enumerate(item_ids_to_plot, start=1):\n",
    "        original_filtered = df[df['item_id'] == item_id]\n",
    "        predictions_filtered = all_preds_df[all_preds_df['item_id'] == item_id]\n",
    "\n",
    "        row = (i-1) // 2 + 1\n",
    "        col = i % 2 if i % 2 != 0 else 2\n",
    "\n",
    "        # Plot the actual values with lines\n",
    "        fig.add_trace(go.Scatter(x=original_filtered['date'], y=original_filtered['new_cases'],\n",
    "                                 mode='lines+markers', name=f'Actual {item_id}',\n",
    "                                 legendgroup=f\"group{i}\", showlegend=False,\n",
    "                                 line=dict(color='blue'),  # Set the line color to blue\n",
    "                                 marker=dict(color=original_filtered['filled_value'].map({True: 'green', False: 'blue'}),  # Color dots based on filled_value\n",
    "                                             size=4)),  # Set marker size\n",
    "                      row=row, col=col)\n",
    "\n",
    "        # Plot the predicted mean\n",
    "        fig.add_trace(go.Scatter(x=predictions_filtered['date'], y=predictions_filtered['pred_mean'],\n",
    "                                 mode='lines', name=f'Predicted {item_id}',\n",
    "                                 legendgroup=f\"group{i}\", showlegend=False, line=dict(color='red')),\n",
    "                      row=row, col=col)\n",
    "\n",
    "        # Prediction intervals\n",
    "        fig.add_trace(go.Scatter(x=predictions_filtered['date'], y=predictions_filtered['pred_lower'],\n",
    "                                 mode='lines', name=f'Lower {item_id}', \n",
    "                                 legendgroup=f\"group{i}\", showlegend=False, line=dict(width=0)),\n",
    "                      row=row, col=col)\n",
    "\n",
    "        fig.add_trace(go.Scatter(x=predictions_filtered['date'], y=predictions_filtered['pred_upper'],\n",
    "                                 mode='lines', name=f'Upper {item_id}', fill='tonexty',\n",
    "                                 legendgroup=f\"group{i}\", showlegend=False, line=dict(width=0), fillcolor='rgba(255, 0, 0, 0.3)'),\n",
    "                      row=row, col=col)\n",
    "        \n",
    "    fig.update_layout(template=\"plotly_dark\", height=300*num_rows, title_text=\"Forecasts of New Cases\", showlegend=True)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_ids = ['ARKANSAS_Chlamydia trachomatis infection', 'ARKANSAS_Gonorrhea',\n",
    "       'CALIFORNIA_Campylobacteriosis',\n",
    "       'CALIFORNIA_Chlamydia trachomatis infection',\n",
    "       'CALIFORNIA_Gonorrhea', 'COLORADO_Chlamydia trachomatis infection',\n",
    "       'COLORADO_Gonorrhea', 'DELAWARE_Chlamydia trachomatis infection',\n",
    "       'FLORIDA_Chlamydia trachomatis infection', 'FLORIDA_Gonorrhea',\n",
    "       'GEORGIA_Chlamydia trachomatis infection', 'GEORGIA_Gonorrhea',\n",
    "       'IDAHO_Chlamydia trachomatis infection',\n",
    "       'ILLINOIS_Chlamydia trachomatis infection',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_forecasts_plotly_by_item_id(df, all_preds_df, item_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_forecasts_plotly_by_item_id(df, all_preds_df, item_ids, num_charts=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_forecasts_plotly_by_item_id(df, all_preds_df, item_ids, num_charts=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evaluation[df_evaluation.new_cases > df_evaluation.pred_upper]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_evaluation[(df_evaluation.new_cases > df_evaluation.pred_upper) & (df_evaluation.date==pd.to_datetime('2023-12-18'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'weekly/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix[0:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
